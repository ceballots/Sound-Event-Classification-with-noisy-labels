{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from librosa.feature import melspectrogram\n",
    "import pandas as pd\n",
    "import soundfile\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join  \n",
    "from librosa.core import stft\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import h5py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_amplitude(y, tolerance=0.005):\n",
    "\n",
    "    mean_value = np.mean(y)\n",
    "    y -= mean_value\n",
    "    max_value = max(abs(y)) + tolerance\n",
    "    return y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2mel(audio,base_path,fs, n_fft,fmax,n_mels,hop_length_samples, window_lenght,ii):\n",
    "    \"\"\"\n",
    "    Convert raw audio to mel spectrogram\n",
    "    \"\"\"\n",
    "    if ii%500 == 0:\n",
    "        print(\"Processing audio number {}\".format(ii))\n",
    "    path = os.path.join(base_path, audio)\n",
    "    data, source_fs = soundfile.read(file=path)\n",
    "    data = data.T\n",
    "    # Resample if the source_fs is different from expected\n",
    "    if fs != source_fs:\n",
    "        data = librosa.core.resample(data, source_fs,fs)\n",
    "    ### extracted from Eduardo Fonseca Code, it seems there are 3 audio corrupted so we need to check length\n",
    "    data = normalize_amplitude(data)\n",
    "\n",
    "    powSpectrum = np.abs(stft(data,n_fft,hop_length = hop_length_samples,\n",
    "                              win_length = window_lenght, window = 'hamming', center=True, pad_mode='reflect'))**2\n",
    "\n",
    "    mels = melspectrogram(y= None,n_fft=n_fft ,sr=fs ,S= powSpectrum, hop_length= hop_length_samples,\n",
    "                          n_mels=n_mels,fmax=fmax , fmin = 0.0).T\n",
    "    \n",
    "    mel_normalized = (mels -  np.mean(mels, axis =0)) / np.amax(mels)\n",
    "    return mel_normalized.T.flatten()\n",
    "\n",
    "def normalize_and_save(list_mels,list_audio_name,which_set,file_name,n_mels = 96):\n",
    "    \"\"\"\n",
    "    Gets mel spectogram, normalize between 0 and 1 and saves it to\n",
    "    a h5 file\n",
    "    :list_mels: mel spectrogram\n",
    "    :list_audio: name of audios\n",
    "    :param which_set: train or test\n",
    "    :param file_name: name of the file to be stored \n",
    "    \"\"\"\n",
    "    \n",
    "    # normalize using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(list_mels)\n",
    "    data_normalized = scaler.transform(list_mels)\n",
    "    \n",
    "    # reshape to (data entries,number of channels, image widht, image height)\n",
    "    data_reshape = np.reshape(data_normalized,\n",
    "                          ((len(data_normalized),1, n_mels,int(data_normalized.shape[1]/n_mels))))\n",
    "    \n",
    "    # get labels and audio names\n",
    "    path = os.path.join(\"FSDnoisy18k.meta\",\"{}_set.csv\".format(which_set))\n",
    "\n",
    "    df_test = pd.read_csv(path)\n",
    "    dict_ = dict(zip(df_test.fname, df_test.label))\n",
    "    \n",
    "    # Encode to save to h5\n",
    "    list_targets_encoded = ([dict_[audio_name].encode('utf8') for audio_name in list_audio_name])\n",
    "    list_audio_encoded = ([audio_name.encode('utf8') for audio_name in list_audio_name])\n",
    "    \n",
    "    # Save it to h5 file\n",
    "    file_name = \"processed_{}_set\".format(which_set)\n",
    "    path = os.path.join(\"DataProcessed\",file_name)\n",
    "    hdf5_name = str(path + '.hdf5')\n",
    "    hdf5_store = h5py.File(hdf5_name, \"w\")\n",
    "    all_inputs = hdf5_store.create_dataset(\"all_inputs\",data = data_reshape, \n",
    "                                           shape = (len(data_normalized),1, n_mels,int(data_normalized.shape[1]/n_mels)),\n",
    "                                           compression=\"gzip\")\n",
    "    dt = h5py.special_dtype(vlen=str)\n",
    "    file_name_ = hdf5_store.create_dataset(\"file_name\", data = list_audio_encoded, \n",
    "                                          dtype=dt ,compression=\"gzip\")\n",
    "    dt = h5py.special_dtype(vlen=str)\n",
    "    targets = hdf5_store.create_dataset(\"targets\", data = list_targets_encoded, \n",
    "                                        dtype=dt ,compression=\"gzip\")\n",
    "    hdf5_store.close()\n",
    "    # Saved\n",
    "    \n",
    "    print(\"{} set has been processed and saved\".format(which_set))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 96\n",
    "audio_duration = 2000 # 2 seconds\n",
    "fs= 32000 # we will make downsampling to save some data!!44100\n",
    "n_fft = 2048\n",
    "windows_size_s = 30 # 30 milisecons windowing (to have more context)\n",
    "windows_size_f = (windows_size_s * fs ) // 1000  # int division # 960 samples\n",
    "hop_length_samples = int(windows_size_f // 2) ## 480 samples\n",
    "number_of_frames = fs * 2 # deprecated, use short audio in database already\n",
    "fmax = int(fs / 2)\n",
    "fmin = 0\n",
    "spectrogram_type = 'power'\n",
    "maximum_mel = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = [f for f in listdir(\"AudioClipsCut/test_audio_cut\")\n",
    "               if isfile(join(\"AudioClipsCut/test_audio_cut\", f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio number 0\n",
      "Processing audio number 500\n",
      "test set has been processed and saved\n"
     ]
    }
   ],
   "source": [
    "list_mels = np.asarray([convert2mel(audio,\"AudioClipsCut/test_audio_cut\",fs, n_fft,fmax,\n",
    "                                    n_mels,hop_length_samples, windows_size_f,ii) \n",
    "                        for ii,audio in enumerate(audio_files)])\n",
    "\n",
    "list_audio_name = ([audio for audio in audio_files])\n",
    "\n",
    "normalize_and_save(list_mels,list_audio_name,'test','processed_test_set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = [f for f in listdir(\"AudioClipsCut/train_audio_cut\")\n",
    "               if isfile(join(\"AudioClipsCut/train_audio_cut\", f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio number 0\n",
      "Processing audio number 500\n",
      "Processing audio number 1000\n",
      "Processing audio number 1500\n",
      "Processing audio number 2000\n",
      "Processing audio number 2500\n",
      "Processing audio number 3000\n",
      "Processing audio number 3500\n",
      "Processing audio number 4000\n",
      "Processing audio number 4500\n",
      "Processing audio number 5000\n",
      "Processing audio number 5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jordi\\Anaconda3\\envs\\py3iaml\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio number 6000\n",
      "Processing audio number 6500\n",
      "Processing audio number 7000\n",
      "Processing audio number 7500\n",
      "Processing audio number 8000\n",
      "Processing audio number 8500\n",
      "Processing audio number 9000\n",
      "Processing audio number 9500\n",
      "Processing audio number 10000\n",
      "Processing audio number 10500\n",
      "Processing audio number 11000\n",
      "Processing audio number 11500\n",
      "Processing audio number 12000\n",
      "Processing audio number 12500\n",
      "Processing audio number 13000\n",
      "Processing audio number 13500\n",
      "Processing audio number 14000\n",
      "Processing audio number 14500\n",
      "Processing audio number 15000\n",
      "Processing audio number 15500\n",
      "Processing audio number 16000\n",
      "Processing audio number 16500\n",
      "Processing audio number 17000\n"
     ]
    }
   ],
   "source": [
    "list_mels = np.asarray([convert2mel(audio,\"AudioClipsCut/train_audio_cut\",fs, n_fft,fmax,\n",
    "                                    n_mels,hop_length_samples, windows_size_f,ii)\n",
    "                        for ii,audio in enumerate(audio_files)])\n",
    "\n",
    "list_cleaned = list_mels[~np.isnan(list_mels).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set has been processed and saved\n"
     ]
    }
   ],
   "source": [
    "list_audio_name = ([audio for audio in audio_files])\n",
    "\n",
    "normalize_and_save(list_cleaned,list_audio_name,'train','processed_train_set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = [f for f in listdir(\"AudioClipsCut/valid_audio_cut\")\n",
    "               if isfile(join(\"AudioClipsCut/valid_audio_cut\", f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio number 0\n"
     ]
    }
   ],
   "source": [
    "list_mels = np.asarray([convert2mel(audio,\"AudioClipsCut/valid_audio_cut\",fs, n_fft,fmax,\n",
    "                                    n_mels,hop_length_samples, windows_size_f,ii)\n",
    "                        for ii,audio in enumerate(audio_files)])\n",
    "\n",
    "list_cleaned = list_mels[~np.isnan(list_mels).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid set has been processed and saved\n"
     ]
    }
   ],
   "source": [
    "list_audio_name = ([audio for audio in audio_files])\n",
    "\n",
    "normalize_and_save(list_cleaned,list_audio_name,'valid','processed_valid_set')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
