{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0f92d94b56e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"FSDnoisy18k.meta/train_set.csv\")\n",
    "df_test = pd.read_csv(\"FSDnoisy18k.meta/test_set.csv\")\n",
    "df_valid = pd.read_csv(\"FSDnoisy18k.meta/valid_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self,\n",
    "                 sampling_rate=16000, audio_duration=2, n_classes=20,\n",
    "                 use_mfcc=False,n_mfcc=20):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.audio_duration = audio_duration\n",
    "        self.use_mfcc = use_mfcc\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.audio_length = self.sampling_rate * self.audio_duration\n",
    "        if self.use_mfcc:\n",
    "            self.dim = (self.n_mfcc *( 1 + int(np.floor(self.audio_length/512))))\n",
    "        else:\n",
    "            self.dim = (self.audio_length, 1)\n",
    "            \n",
    "    def prepare_data(self,df,data_dir):\n",
    "        X = np.empty(shape=(df.shape[0],self.dim))\n",
    "        y = []\n",
    "        f_name = []\n",
    "        input_length = self.audio_length\n",
    "        for i, (fname,label) in enumerate(zip(df.fname,df.label)):\n",
    "            file_path = data_dir + fname\n",
    "            data, _ = librosa.core.load(file_path, sr=self.sampling_rate, res_type=\"kaiser_fast\")\n",
    "\n",
    "            # Random offset / Padding\n",
    "            if len(data) > input_length:\n",
    "                max_offset = len(data) - input_length\n",
    "                offset = np.random.randint(max_offset)\n",
    "                data = data[offset:(input_length+offset)]\n",
    "            else:\n",
    "                if input_length > len(data):\n",
    "                    max_offset = input_length - len(data)\n",
    "                    offset = np.random.randint(max_offset)\n",
    "                else:\n",
    "                    offset = 0\n",
    "                data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
    "\n",
    "            data = librosa.feature.mfcc(data, sr=self.sampling_rate, n_mfcc=self.n_mfcc)\n",
    "            data = data.reshape((self.dim))\n",
    "            X[i,] = data\n",
    "            y.append(label)\n",
    "            f_name.append(fname)\n",
    "        return X,y,f_name\n",
    "    \n",
    "    def normalize_and_save(self,list_mels,y,list_fname,which_set):\n",
    "        \"\"\"\n",
    "        Gets mel spectogram, normalize between 0 and 1 and saves it to\n",
    "        a h5 file\n",
    "        :list_mels: mel spectrogram\n",
    "        :list_audio: name of audios\n",
    "        :param which_set: train or test\n",
    "        \"\"\"\n",
    "        # remove weird values\n",
    "        list_mels = list_mels[~np.isnan(list_mels).any(axis=1)]\n",
    "        \n",
    "        # normalize using MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(list_mels)\n",
    "        data_normalized = scaler.transform(list_mels)\n",
    "\n",
    "        # reshape to (data entries,number of channels, image widht, image height)\n",
    "        data_reshape = np.reshape(data_normalized,\n",
    "                              ((len(data_normalized),1, self.n_mfcc,1 + int(np.floor(self.audio_length/512)))))\n",
    "\n",
    "        # get labels and audio names\n",
    "\n",
    "        # Encode to save to h5\n",
    "        \n",
    "        list_targets_encoded = ([y_.encode('utf8') for y_ in y])\n",
    "        list_audio_encoded = ([audio_name.encode('utf8') for audio_name in list_fname])\n",
    "\n",
    "        # Save it to h5 file\n",
    "        file_name = \"processed_{}_set\".format(which_set)\n",
    "        path = os.path.join(\"DataProcessed\",file_name)\n",
    "        hdf5_name = str(path + '.hdf5')\n",
    "        hdf5_store = h5py.File(hdf5_name, \"w\")\n",
    "        all_inputs = hdf5_store.create_dataset(\"all_inputs\",data = data_reshape, \n",
    "                                               shape = ((len(data_normalized),1, self.n_mfcc,1 + int(np.floor(self.audio_length/512)))),\n",
    "                                               compression=\"gzip\")\n",
    "        dt = h5py.special_dtype(vlen=str)\n",
    "        file_name_ = hdf5_store.create_dataset(\"file_name\", data = list_audio_encoded, \n",
    "                                              dtype=dt ,compression=\"gzip\")\n",
    "        dt = h5py.special_dtype(vlen=str)\n",
    "        targets = hdf5_store.create_dataset(\"targets\", data = list_targets_encoded, \n",
    "                                            dtype=dt ,compression=\"gzip\")\n",
    "        hdf5_store.close()\n",
    "        # Saved\n",
    "\n",
    "        print(\"{} set has been processed and saved\".format(which_set)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid set has been processed and saved\n"
     ]
    }
   ],
   "source": [
    "config_valid = Config(sampling_rate=44100, audio_duration=2, \n",
    "                use_mfcc=True, n_mfcc=40)\n",
    "X,y,f_name = config_valid.prepare_data(df_valid,'../AudioClipsCut/valid_audio_cut/')\n",
    "config_valid.normalize_and_save(X,y,f_name,'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set has been processed and saved\n"
     ]
    }
   ],
   "source": [
    "config_test = Config(sampling_rate=44100, audio_duration=2, \n",
    "                use_mfcc=True, n_mfcc=40)\n",
    "X,y,f_name = config_test.prepare_data(df_test,'../AudioClipsCut/test_audio_cut/')\n",
    "config_test.normalize_and_save(X,y,f_name,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set has been processed and saved\n"
     ]
    }
   ],
   "source": [
    "config_train = Config(sampling_rate=44100, audio_duration=2, \n",
    "                use_mfcc=True, n_mfcc=40)\n",
    "X,y,f_name = config_train.prepare_data(df_train,'../AudioClipsCut/train_audio_cut/')\n",
    "config_train.normalize_and_save(X,y,f_name,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jordi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "file=h5py.File(\"DataProcessed/processed_valid_set.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = file['all_inputs'][:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 1, 40, 173)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
